{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importar las librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms import transforms\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determinar si se usa CUDA o CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available(): \n",
    " print(\"cuda\") \n",
    "else: \n",
    " print(\"cpu\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definir la ubicaci贸n del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path = pathlib.Path(__file__).parent.absolute() # Para .py\n",
    "file_path = Path.cwd() # Para .ipnyb\n",
    "dataset_path = base_folder = pathlib.Path(file_path) / \"data/Dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definir las transformaciones que se realizan en entrenamiento, validaci贸n y en ambas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(split):\n",
    "    global_transforms = [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Grayscale(),\n",
    "        torchvision.transforms.Resize((48, 48))\n",
    "    ]\n",
    "    \n",
    "    mean, std = 0.5, 0.5\n",
    "\n",
    "    train_transforms = transforms.Compose([\n",
    "        *global_transforms,\n",
    "        #Data augmentation\n",
    "        torchvision.transforms.Normalize((mean,), (std,))\n",
    "    ])\n",
    "\n",
    "    test_transforms = transforms.Compose([\n",
    "        *global_transforms,\n",
    "        torchvision.transforms.Normalize((mean,), (std,))\n",
    "    ])\n",
    "\n",
    "    if split == \"train\":\n",
    "        return train_transforms\n",
    "    else:\n",
    "        return test_transforms\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear la clase Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EyeDataset(Dataset):\n",
    "    def __init__(self, root_dir, split = \"train\"):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = get_transforms(self.split)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.root_dir)\n",
    "        image = read_image(img_path)\n",
    "        image_name = image_path.split('/')[-1]\n",
    "        label = image_name[16]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return (image, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.root_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear el dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(batch_size, shuffle=True, num_workers=0):\n",
    "\n",
    "    train_dataset = EyeDataset(root_dir = dataset_path, split = \"train\")\n",
    "    val_dataset = EyeDataset(root_dir = dataset_path, split = \"val\")\n",
    "\n",
    "    total_len = len(train_dataset)\n",
    "    \n",
    "    indices = torch.randperm(len(train_dataset)) # Sortea los indices\n",
    "    val_size = total_len//3 # Un tercio del total aplicado piso\n",
    "    train_dataset = torch.utils.data.Subset(train_dataset, indices[:-val_size]) # Selecciona todos los indices menos los ultimos val_size indices\n",
    "    val_dataset = torch.utils.data.Subset(val_dataset, indices[-val_size:]) # Selecciona solo los ultimos val_size indices\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=shuffle,\n",
    "                num_workers=num_workers,\n",
    "        )\n",
    "\n",
    "    val_dataloader = DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=shuffle,\n",
    "                num_workers=num_workers,\n",
    "        )\n",
    "\n",
    "\n",
    "    return train_dataset, train_dataloader, val_dataset, val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creaci贸n de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, input_dim: int, n_classes: int) -> None:\n",
    "        super().__init__()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        out_dim =  self.calc_out_dim(input_dim,5, padding=2)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1,16,kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(16,32,kernel_size=5)\n",
    "        self.lineal1 = nn.Linear(32*self.calc_out_dim(out_dim,5)*self.calc_out_dim(out_dim,5),1024)\n",
    "        self.lineal2 = nn.Linear(1024,n_classes)\n",
    "        \n",
    "        self.to(self.device)\n",
    " \n",
    "    def calc_out_dim(self, in_dim, kernel_size, stride=1, padding=0):\n",
    "        out_dim = math.floor((in_dim - kernel_size + 2*padding)/stride) + 1\n",
    "        return out_dim\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.lineal1(x)\n",
    "        x = F.relu(x)\n",
    "        logits = self.lineal2(x)\n",
    "        proba = F.softmax(logits, dim=1)\n",
    "\n",
    "        return logits, proba\n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.inference_mode():\n",
    "            return self.forward(x)\n",
    "\n",
    "    def save_model(self, model_name: str):\n",
    "        models_path = file_path / 'models' / model_name\n",
    "        torch.save(self.state_dict(),models_path)\n",
    "\n",
    "    def load_model(self, model_name: str):\n",
    "        self.load_state_dict(torch.load(file_path / 'models' / model_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from network import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pathlib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "file_path = Path.cwd()\n",
    "\n",
    "class PlotLosses():\n",
    "    def __init__(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, train_loss, val_loss):        \n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(train_loss)\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.i += 1\n",
    "        plt.cla()\n",
    "        plt.plot(self.x, self.losses, label=\"Costo de entrenamiento promedio\")\n",
    "        plt.plot(self.x, self.val_losses, label=\"Costo de validaci贸n promedio\")\n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show(block=False)\n",
    "        plt.pause(5)\n",
    "\n",
    "    def on_train_end(self):\n",
    "        plt.show()\n",
    "        today = dt.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        losses_file = file_path/ f'figures/losses_{today}.png'\n",
    "        plt.savefig(losses_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(val_loader, net, cost_function):\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, batch in enumerate(val_loader, 0):\n",
    "        batch_imgs = batch['transformed']\n",
    "        batch_labels = batch['label']\n",
    "        if torch.cuda.is_available():\n",
    "            batch_labels = batch_labels.cuda()\n",
    "        with torch.inference_mode():\n",
    "            outputs,proba = net(batch_imgs)\n",
    "            loss = cost_function(outputs, batch_labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            predictions = torch.argmax(proba,dim=1)\n",
    "            total += batch_labels.size(0)\n",
    "            cor = (predictions == batch_labels).sum().item()\n",
    "            correct += cor\n",
    "\n",
    "    accuracy = 100 * float(correct) / total\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    return val_loss/len(val_loader)\n",
    "\n",
    "def train():\n",
    "    learning_rate = 1e-5\n",
    "    n_epochs= 50\n",
    "    batch_size = 32\n",
    "\n",
    "    train_dataset, train_loader, _, _ = \\\n",
    "        get_loader(batch_size=batch_size,\n",
    "                    shuffle=True)\n",
    "    _, _, val_dataset, val_loader = \\\n",
    "        get_loader(batch_size=batch_size,\n",
    "                    shuffle=False)\n",
    "\n",
    "    print(f\"Cargando datasets --> entrenamiento: {len(train_dataset)}, validacion: {len(val_dataset)}\")\n",
    "\n",
    "    plotter = PlotLosses()\n",
    "    modelo = Network(input_dim = 48,\n",
    "                     n_classes = 7)\n",
    "\n",
    "    cost_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.Adam(modelo.parameters(), learning_rate, weight_decay=0.01)\n",
    "    best_epoch_loss = np.inf\n",
    "    best_epoch_loss_train = np.inf\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0\n",
    "        for i, batch in enumerate(tqdm(train_loader, desc=f\"Epoch: {epoch}\")):\n",
    "            batch_imgs = batch['transformed']\n",
    "            batch_labels = batch['label']\n",
    "            if torch.cuda.is_available():\n",
    "                batch_labels = batch_labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            preds,_ = modelo(batch_imgs)\n",
    "            loss = cost_function(preds, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss = train_loss/len(train_loader)\n",
    "        val_loss = validation_step(val_loader, modelo, cost_function)\n",
    "        tqdm.write(f\"Epoch: {epoch}, train_loss: {train_loss:.2f}, val_loss: {val_loss:.2f}\")\n",
    "\n",
    "        if(val_loss<best_epoch_loss):\n",
    "            modelo.save_model(\"modelo_val_1.pt\")\n",
    "            best_epoch_loss=val_loss\n",
    "        if(train_loss<best_epoch_loss_train):\n",
    "            modelo.save_model(\"modelo_ent_1.pt\")\n",
    "            best_epoch_loss=train_loss\n",
    "        plotter.on_epoch_end(epoch, train_loss, val_loss)\n",
    "    modelo.save_model(\"modelo_fin_1.pt\")\n",
    "    plotter.on_train_end()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datasets --> entrenamiento: 56599, validacion: 28299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0:   0%|          | 0/1769 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "unable to mmap 4403200 bytes from file </home/hij555/Documents/GPT-5/ProyectoFinal/ML23_GPT5/ml23/proyecto_final/Backend/data/Dataset>: No such device (19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/hij555/Documents/GPT-5/ProyectoFinal/ML23_GPT5/ml23/proyecto_final/Backend/Prueba.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hij555/Documents/GPT-5/ProyectoFinal/ML23_GPT5/ml23/proyecto_final/Backend/Prueba.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train()\n",
      "\u001b[1;32m/home/hij555/Documents/GPT-5/ProyectoFinal/ML23_GPT5/ml23/proyecto_final/Backend/Prueba.ipynb Cell 20\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hij555/Documents/GPT-5/ProyectoFinal/ML23_GPT5/ml23/proyecto_final/Backend/Prueba.ipynb#X24sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_epochs):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hij555/Documents/GPT-5/ProyectoFinal/ML23_GPT5/ml23/proyecto_final/Backend/Prueba.ipynb#X24sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/hij555/Documents/GPT-5/ProyectoFinal/ML23_GPT5/ml23/proyecto_final/Backend/Prueba.ipynb#X24sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(train_loader, desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hij555/Documents/GPT-5/ProyectoFinal/ML23_GPT5/ml23/proyecto_final/Backend/Prueba.ipynb#X24sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m         batch_imgs \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mtransformed\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hij555/Documents/GPT-5/ProyectoFinal/ML23_GPT5/ml23/proyecto_final/Backend/Prueba.ipynb#X24sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m         batch_labels \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.8/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset, \u001b[39m\"\u001b[39m\u001b[39m__getitems__\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.8/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices])  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[idx]] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.8/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices])  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices]\n",
      "\u001b[1;32m/home/hij555/Documents/GPT-5/ProyectoFinal/ML23_GPT5/ml23/proyecto_final/Backend/Prueba.ipynb Cell 20\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hij555/Documents/GPT-5/ProyectoFinal/ML23_GPT5/ml23/proyecto_final/Backend/Prueba.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hij555/Documents/GPT-5/ProyectoFinal/ML23_GPT5/ml23/proyecto_final/Backend/Prueba.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     img_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_dir)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hij555/Documents/GPT-5/ProyectoFinal/ML23_GPT5/ml23/proyecto_final/Backend/Prueba.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     image \u001b[39m=\u001b[39m read_image(img_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hij555/Documents/GPT-5/ProyectoFinal/ML23_GPT5/ml23/proyecto_final/Backend/Prueba.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     image_name \u001b[39m=\u001b[39m image_path\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hij555/Documents/GPT-5/ProyectoFinal/ML23_GPT5/ml23/proyecto_final/Backend/Prueba.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     label \u001b[39m=\u001b[39m image_name[\u001b[39m16\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.8/site-packages/torchvision/io/image.py:258\u001b[0m, in \u001b[0;36mread_image\u001b[0;34m(path, mode)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_tracing():\n\u001b[1;32m    257\u001b[0m     _log_api_usage_once(read_image)\n\u001b[0;32m--> 258\u001b[0m data \u001b[39m=\u001b[39m read_file(path)\n\u001b[1;32m    259\u001b[0m \u001b[39mreturn\u001b[39;00m decode_image(data, mode)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.8/site-packages/torchvision/io/image.py:52\u001b[0m, in \u001b[0;36mread_file\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_tracing():\n\u001b[1;32m     51\u001b[0m     _log_api_usage_once(read_file)\n\u001b[0;32m---> 52\u001b[0m data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mimage\u001b[39m.\u001b[39;49mread_file(path)\n\u001b[1;32m     53\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.8/site-packages/torch/_ops.py:692\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    688\u001b[0m     \u001b[39m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    689\u001b[0m     \u001b[39m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     \u001b[39m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    691\u001b[0m     \u001b[39m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs \u001b[39mor\u001b[39;49;00m {})\n",
      "\u001b[0;31mRuntimeError\u001b[0m: unable to mmap 4403200 bytes from file </home/hij555/Documents/GPT-5/ProyectoFinal/ML23_GPT5/ml23/proyecto_final/Backend/data/Dataset>: No such device (19)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
